{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "# Load the data: train, test.csv\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Assumed the sequence is main feature\n",
        "X_train = train_data[\"sequence\"].values\n",
        "y_train = train_data[\"target\"].values\n",
        "X_test = test_data[\"sequence\"].values\n",
        "test_ids = test_data[\"id\"].values\n",
        "\n",
        "# Tokenized the sequences to model\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Converting the sequences to sequences of integers here:\n",
        "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding the sequences to make sure they have the same length\n",
        "max_sequence_length = max(max(len(seq) for seq in X_train_tokenized), max(len(seq) for seq in X_test_tokenized))\n",
        "X_train_padded = pad_sequences(X_train_tokenized, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_tokenized, maxlen=max_sequence_length)\n",
        "\n",
        "# Preprocessing: Standarding the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_padded)\n",
        "X_test_scaled = scaler.transform(X_test_padded)\n",
        "\n",
        "# Reshaping the input data for LSTM\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "# Defining the model builder function for Keras Tuner\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('units_lstm1', min_value=64, max_value=512, step=32),\n",
        "                                 input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]),\n",
        "                                 return_sequences=True, kernel_regularizer=l2(0.001))))\n",
        "    model.add(Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.6, step=0.1)))\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('units_lstm2', min_value=64, max_value=512, step=32),\n",
        "                                 return_sequences=True, kernel_regularizer=l2(0.001))))\n",
        "    model.add(Dropout(hp.Float('dropout2', min_value=0.2, max_value=0.6, step=0.1)))\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('units_lstm3', min_value=64, max_value=512, step=32),\n",
        "                                 kernel_regularizer=l2(0.001))))\n",
        "    model.add(Dropout(hp.Float('dropout3', min_value=0.2, max_value=0.6, step=0.1)))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    # Compiling the model\n",
        "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# Instantiate the RandomSearch tuner with enhanced:\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    # Optimizing for validation loss\n",
        "    max_trials=20,\n",
        "    # Increasing number of trials\n",
        "    executions_per_trial=2,\n",
        "    # Increasing number of executions per trial\n",
        "    directory='keras_tuner_results',\n",
        "    project_name='hyperparameter_tuning')\n",
        "\n",
        "# Performing hyperparameter tuning\n",
        "tuner.search(X_train_reshaped, y_train,\n",
        "             epochs=100,\n",
        "             validation_split=0.2,\n",
        "             verbose=1)\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Making predictions\n",
        "predictions = best_model.predict(X_test_reshaped)\n",
        "\n",
        "# Creating a DataFrame for predictions with id column\n",
        "prediction_df = pd.DataFrame({\"id\": test_ids, \"target\": predictions.flatten()})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "prediction_df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79FKfyyw9bmZ",
        "outputId": "ce97686a-85cd-43e5-c460-ec59656db655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 20 Complete [00h 11m 04s]\n",
            "val_loss: 18.942358016967773\n",
            "\n",
            "Best val_loss So Far: 18.63950538635254\n",
            "Total elapsed time: 02h 42m 22s\n",
            "61/61 [==============================] - 5s 4ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}